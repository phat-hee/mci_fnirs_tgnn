{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e668d5e-490f-49e4-9db2-e10634b34be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "\n",
    "# Paths to the directories containing the .pt files\n",
    "hc_path = '/content/drive/MyDrive/removedspikes_nonan_replace_window10avg/ply_dtrnd/dtrnd-tddr-fit-ply-dtrnd/baselinecorr/ready for segmentation/merged_datas_all_normalized(abslute value max each ch)/whole2_allfunctions_hc_rest_whole_swn+graphfeat+hbr'\n",
    "mci_path = '/content/drive/MyDrive/removedspikes_nonan_replace_window10avg/ply_dtrnd/dtrnd-tddr-fit-ply-dtrnd/baselinecorr/ready for segmentation/merged_datas_all_normalized(abslute value max each ch)/whole2_allfunctions_mci_rest_whole_swn+graphfeat+hbr'\n",
    "\n",
    "# Function to load .pt files from a directory and extract node and edge features\n",
    "def load_data_from_directory(directory, label):\n",
    "    data_list = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.pt'):\n",
    "            data = torch.load(os.path.join(directory, filename))\n",
    "            node_features = data.x.flatten().numpy()\n",
    "            edge_features = data.edge_attr.flatten().numpy()\n",
    "            combined_features = np.concatenate((node_features, edge_features))\n",
    "            data_list.append((combined_features, label))\n",
    "    return data_list\n",
    "\n",
    "# Load data from both directories\n",
    "hc_data = load_data_from_directory(hc_path, 0)\n",
    "mci_data = load_data_from_directory(mci_path, 1)\n",
    "\n",
    "# Combine the data and create feature matrix and labels\n",
    "all_data = hc_data + mci_data\n",
    "\n",
    "# Find the maximum length of the feature vectors\n",
    "max_length = max(len(item[0]) for item in all_data)\n",
    "\n",
    "# Pad feature vectors to the maximum length\n",
    "X = np.array([np.pad(item[0], (0, max_length - len(item[0])), 'constant') for item in all_data])\n",
    "y = np.array([item[1] for item in all_data])\n",
    "\n",
    "# Get the shape of the original data\n",
    "sample_data = torch.load(os.path.join(hc_path, os.listdir(hc_path)[2]))\n",
    "num_nodes = sample_data.x.shape[0]\n",
    "num_node_features = sample_data.x.shape[1]\n",
    "num_edge_features = sample_data.edge_attr.shape[1]\n",
    "total_node_features = num_nodes * num_node_features\n",
    "\n",
    "# Function to evaluate model and get feature importances\n",
    "def evaluate_model(X_train, X_test, y_train, y_test, random_state):\n",
    "    model = xgb.XGBClassifier(n_estimators=100, random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "\n",
    "    feature_importance = model.feature_importances_\n",
    "    node_importance = feature_importance[:total_node_features].reshape(num_nodes, num_node_features).sum(axis=0)\n",
    "    edge_importance = feature_importance[total_node_features:].reshape(-1, num_edge_features).sum(axis=0)\n",
    "\n",
    "    return accuracy, f1, recall, node_importance, edge_importance\n",
    "\n",
    "# Cross-validation\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=42)\n",
    "model = xgb.XGBClassifier(n_estimators=100)\n",
    "cv_accuracy_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "cv_f1_scores = cross_val_score(model, X, y, cv=cv, scoring='f1')\n",
    "cv_recall_scores = cross_val_score(model, X, y, cv=cv, scoring='recall')\n",
    "\n",
    "print(f\"Cross-validation accuracy: {cv_accuracy_scores.mean():.4f} (+/- {cv_accuracy_scores.std() * 2:.4f})\")\n",
    "print(f\"Cross-validation F1 score: {cv_f1_scores.mean():.4f} (+/- {cv_f1_scores.std() * 2:.4f})\")\n",
    "print(f\"Cross-validation recall: {cv_recall_scores.mean():.4f} (+/- {cv_recall_scores.std() * 2:.4f})\")\n",
    "\n",
    "# Multiple runs with different random states\n",
    "n_runs = 10\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "recalls = []\n",
    "node_importances = []\n",
    "edge_importances = []\n",
    "\n",
    "for i in range(n_runs):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i, stratify=y)\n",
    "    accuracy, f1, recall, node_importance, edge_importance = evaluate_model(X_train, X_test, y_train, y_test, i)\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    recalls.append(recall)\n",
    "    node_importances.append(node_importance)\n",
    "    edge_importances.append(edge_importance)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nAverage accuracy: {np.mean(accuracies):.4f} (+/- {np.std(accuracies):.4f})\")\n",
    "print(f\"Average F1 score: {np.mean(f1_scores):.4f} (+/- {np.std(f1_scores):.4f})\")\n",
    "print(f\"Average recall: {np.mean(recalls):.4f} (+/- {np.std(recalls):.4f})\")\n",
    "\n",
    "print(\"\\nAverage Node Feature Importances:\")\n",
    "for i, importance in enumerate(np.mean(node_importances, axis=0)):\n",
    "    print(f\"Node Feature {i+1}: {importance:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Edge Feature Importances:\")\n",
    "for i, importance in enumerate(np.mean(edge_importances, axis=0)):\n",
    "    print(f\"Edge Feature {i+1}: {importance:.4f}\")\n",
    "\n",
    "# Identify most important features\n",
    "node_importance_mean = np.mean(node_importances, axis=0)\n",
    "edge_importance_mean = np.mean(edge_importances, axis=0)\n",
    "\n",
    "top_node_features = np.argsort(node_importance_mean)[-4:][::-1]\n",
    "top_edge_features = np.argsort(edge_importance_mean)[-3:][::-1]\n",
    "\n",
    "print(\"\\nTop 3 Most Important Node Features:\")\n",
    "for i, feature in enumerate(top_node_features):\n",
    "    print(f\"{i+1}. Node Feature {feature+1}: {node_importance_mean[feature]:.4f}\")\n",
    "\n",
    "print(\"\\nTop 3 Most Important Edge Features:\")\n",
    "for i, feature in enumerate(top_edge_features):\n",
    "    print(f\"{i+1}. Edge Feature {feature+1}: {edge_importance_mean[feature]:.4f}\")\n",
    "\n",
    "node_feature_names = [\n",
    "    'HbO Max', 'HbO Min', 'HbO Mean', 'HbO Std',\n",
    "    'HbO Slope', 'HbO Wavelet',\n",
    "    'HbR Max', 'HbR Min', 'HbR Mean', 'HbR Std',\n",
    "    'HbR Slope', 'HbR Wavelet'\n",
    "]\n",
    "\n",
    "# Edge feature names\n",
    "edge_feature_names = [\n",
    "    'Covariance', 'Pearson Correlation', 'Spearman Correlation',\n",
    "    'Kendall Tau', 'Distance Correlation', ' Dynamic Time Wraping Distance', 'PLI',\n",
    "    'Coherence', 'PLV' , 'Cross Correlation'\n",
    "]\n",
    "\n",
    "# Calculate mean importances\n",
    "node_importance_mean = np.mean(node_importances, axis=0)\n",
    "edge_importance_mean = np.mean(edge_importances, axis=0)\n",
    "\n",
    "def plot_feature_importance(feature_names, importances, title):\n",
    "    # Check if the number of feature names matches the number of importances\n",
    "    if len(feature_names) != len(importances):\n",
    "        print(f\"Warning: Number of feature names ({len(feature_names)}) doesn't match the number of importances ({len(importances)}).\")\n",
    "        feature_names = [f\"Feature {i+1}\" for i in range(len(importances))]\n",
    "\n",
    "    # Sort features by importance\n",
    "    sorted_idx = np.argsort(importances)[::-1]  # Sort in descending order\n",
    "    sorted_names = [feature_names[i] for i in sorted_idx]\n",
    "    sorted_importances = importances[sorted_idx]\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(range(len(sorted_importances)), sorted_importances)\n",
    "\n",
    "    # Set colors and labels\n",
    "    for bar in bars:\n",
    "        bar.set_color('skyblue')\n",
    "    plt.yticks(range(len(sorted_importances)), sorted_names)\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title(title)\n",
    "\n",
    "    # Adjust layout and display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print top 5 features\n",
    "    print(f\"\\nTop 5 Most Important {title}:\")\n",
    "    for i in range(min(5, len(sorted_names))):\n",
    "        print(f\"{i+1}. {sorted_names[i]}: {sorted_importances[i]:.4f}\")\n",
    "\n",
    "# Plot node features\n",
    "plot_feature_importance(node_feature_names, node_importance_mean, \"Node Feature Importance for MCI vs HC Classification\")\n",
    "\n",
    "# Plot edge features\n",
    "plot_feature_importance(edge_feature_names, edge_importance_mean, \"Edge Feature Importance for MCI vs HC Classification\")\n",
    "\n",
    "# Print the shapes of importances for debugging\n",
    "print(f\"\\nShape of node_importance_mean: {node_importance_mean.shape}\")\n",
    "print(f\"Shape of edge_importance_mean: {edge_importance_mean.shape}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
