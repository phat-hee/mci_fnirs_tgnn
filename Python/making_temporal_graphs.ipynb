{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omcAFIlJQgsi"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.data import Data\n",
        "from torch_geometric_temporal.signal import StaticGraphTemporalSignal\n",
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy.signal import hilbert, coherence\n",
        "import scipy.io\n",
        "import networkx as nx\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
        "import dcor\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "import pywt\n",
        "\n",
        "\n",
        "def calculate_small_worldness(G):\n",
        "    if not nx.is_connected(G):\n",
        "        largest_cc = max(nx.connected_components(G), key=len)\n",
        "        G = G.subgraph(largest_cc).copy()\n",
        "\n",
        "    n = G.number_of_nodes()\n",
        "    m = G.number_of_edges()\n",
        "\n",
        "    if n < 2:\n",
        "        return np.nan\n",
        "\n",
        "    avg_clustering = nx.average_clustering(G)\n",
        "    avg_path_length = nx.average_shortest_path_length(G)\n",
        "\n",
        "    expected_clustering = 2 * m / (n * (n - 1))\n",
        "    expected_path_length = np.log(n) / np.log((2 * m) / n)\n",
        "\n",
        "    if expected_clustering == 0 or expected_path_length == 0:\n",
        "        return np.nan\n",
        "\n",
        "    small_worldness = (avg_clustering / expected_clustering) / (avg_path_length / expected_path_length)\n",
        "    return small_worldness\n",
        "\n",
        "def calculate_modularity(G):\n",
        "    partition = nx.community.greedy_modularity_communities(G)\n",
        "    return nx.community.modularity(G, partition)\n",
        "\n",
        "def calculate_global_efficiency(G):\n",
        "    return nx.global_efficiency(G)\n",
        "\n",
        "def calculate_avg_clustering_coefficient(G):\n",
        "    return nx.average_clustering(G)\n",
        "\n",
        "def calculate_eigenvector_centrality(G):\n",
        "    try:\n",
        "        centrality = nx.eigenvector_centrality(G)\n",
        "        return np.mean(list(centrality.values()))\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def calculate_wavelet_coeffs(signal, level=6):\n",
        "    coeffs = pywt.wavedec(signal, 'db4', level=level)\n",
        "    return np.concatenate(coeffs)\n",
        "\n",
        "def calculate_covariance(signal1, signal2):\n",
        "    return np.cov(signal1, signal2)[0, 1]\n",
        "\n",
        "def calculate_phase_lag_index(signal1, signal2):\n",
        "    analytic_signal1 = hilbert(signal1)\n",
        "    analytic_signal2 = hilbert(signal2)\n",
        "    instantaneous_phase1 = np.angle(analytic_signal1)\n",
        "    instantaneous_phase2 = np.angle(analytic_signal2)\n",
        "    phase_difference = instantaneous_phase1 - instantaneous_phase2\n",
        "    pli = abs(np.mean(np.sign(np.sin(phase_difference))))\n",
        "    return pli\n",
        "\n",
        "def calculate_cross_correlation(signal1, signal2):\n",
        "    cross_corr = np.correlate(signal1, signal2, mode='full')\n",
        "    return np.mean(cross_corr)\n",
        "\n",
        "def calculate_pearson_correlation(x, y):\n",
        "    return pearsonr(x, y)[0]\n",
        "\n",
        "\n",
        "input_directory = '/content/drive/MyDrive/removedspikes_nonan_replace_window10avg/ply_dtrnd/dtrnd-tddr-fit-ply-dtrnd/baselinecorr/ready for segmentation/merged_datas_all_normalized(abslute value max each ch)/rest/mci'\n",
        "output_directory = '/content/drive/MyDrive/removedspikes_nonan_replace_window10avg/ply_dtrnd/dtrnd-tddr-fit-ply-dtrnd/baselinecorr/ready for segmentation/merged_datas_all_normalized(abslute value max each ch)/temporal2_10_rest_mci_swn+graphfeat+hbr/left'\n",
        "\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "regions = {\n",
        "    'right': list(range(16)),\n",
        "    'middle': list(range(16, 32)),\n",
        "    'left': list(range(32, 48)),\n",
        "    'all': list(range(48))\n",
        "}\n",
        "\n",
        "\n",
        "# channels_of_interest = [2, 3, 4, 6, 7, 9, 14, 25, 30, 31, 33, 36, 39, 40, 47, 48] #16_roi_channel\n",
        "channels_of_interest =[33,34,35, 36,37,38, 39, 40,41,42,43,44,45,46 ,47, 48] #[1,2, 3, 4,5, 6, 7,8, 9,10,11,12,13, 14,15,16]  # [17,18,19,20,21,22,23,24 ,25,26,27,28,29, 30, 31,32]# # #[33,34,35, 36,37,38, 39, 40,41,42,43,44,45,46 ,47, 48]# # #  ##  ##\n",
        "\n",
        "#[33,34,35, 36,37,38, 39, 40,41,42,43,44,45,46 ,47, 48]\n",
        "\n",
        "# channels_of_interest = [2, 3, 4, 6, 7, 9, 14, 25, 30, 31, 33, 36, 39, 40, 47, 48] #16_roi_channel\n",
        "#channels_of_interest = [1,2, 3, 4,5, 6, 7,8, 9,10,11,12,13, 14,15,16,17,18,19,20,21,22,23,24 ,25,26,27,28,29, 30, 31,32, 33,34,35, 36,37,38, 39, 40,41,42,43,44,45,46 ,47, 48] #16_roi_channel\n",
        "channels_of_interest = [ch - 1 for ch in channels_of_interest]\n",
        "\n",
        "# Prepare the sequence of temporal graphs\n",
        "temporal_graphs = []\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the sequence of temporal graphs for each patient\n",
        "for filename in os.listdir(input_directory):\n",
        "    if filename.endswith('_hbo_denoised_cleaned_cleaned_filtered.mat'):\n",
        "        base_filename = filename.replace('_hbo_denoised_cleaned_cleaned_filtered.mat', '')\n",
        "        hbo_file_path = os.path.join(input_directory, filename)\n",
        "        hbr_file_path = os.path.join(input_directory, f'{base_filename}_hbr_denoised_cleaned_cleaned_filtered.mat')\n",
        "\n",
        "        if os.path.exists(hbr_file_path):\n",
        "            hbo_mat = scipy.io.loadmat(hbo_file_path)\n",
        "            hbo_data = hbo_mat['corrected_data']\n",
        "\n",
        "            hbr_mat = scipy.io.loadmat(hbr_file_path)\n",
        "            hbr_data = hbr_mat['corrected_data']\n",
        "\n",
        "            window_size = int(8.138 * 10)  # 30 seconds window size\n",
        "            overlap = 0\n",
        "\n",
        "            start_index = int(8.138 * 5)\n",
        "            end_index = len(hbo_data) - int(8.138 * 0)\n",
        "            num_windows = 1 + (end_index - start_index - window_size) // (window_size - overlap)\n",
        "            pbar = tqdm(total=num_windows, desc=f\"Processing {filename}\", unit=\"window\")\n",
        "\n",
        "            temporal_graphs = []\n",
        "\n",
        "            i = start_index\n",
        "            while i < end_index:\n",
        "                edge_index = []\n",
        "                edge_attr = []\n",
        "                x = []\n",
        "\n",
        "                if i + window_size > len(hbo_data):\n",
        "                    hbo_window_data = hbo_data[-window_size:, :]\n",
        "                    hbr_window_data = hbr_data[-window_size:, :]\n",
        "                else:\n",
        "                    hbo_window_data = hbo_data[i:i + window_size, :]\n",
        "                    hbr_window_data = hbr_data[i:i + window_size, :]\n",
        "\n",
        "                for j in channels_of_interest:\n",
        "                    if np.all(hbo_window_data[:, j] == 0) or np.all(hbr_window_data[:, j] == 0):\n",
        "                        continue\n",
        "\n",
        "                    hbo_signal = hbo_window_data[:, j]\n",
        "                    hbr_signal = hbr_window_data[:, j]\n",
        "\n",
        "                    # Node features for HbO\n",
        "                    hbo_max = np.max(hbo_signal)\n",
        "                    hbo_min = np.min(hbo_signal)\n",
        "                    hbo_mean = np.mean(hbo_signal)\n",
        "                    hbo_std = np.std(hbo_signal)\n",
        "                    hbo_slope = np.polyfit(np.arange(len(hbo_signal)), hbo_signal, 1)[0]\n",
        "                    hbo_wavelet = calculate_wavelet_coeffs(hbo_signal)\n",
        "                    hbo_wavelet_mean = np.mean(hbo_wavelet)\n",
        "\n",
        "                    # Node features for HbR\n",
        "                    hbr_max = np.max(hbr_signal)\n",
        "                    hbr_min = np.min(hbr_signal)\n",
        "                    hbr_mean = np.mean(hbr_signal)\n",
        "                    hbr_std = np.std(hbr_signal)\n",
        "                    hbr_slope = np.polyfit(np.arange(len(hbr_signal)), hbr_signal, 1)[0]\n",
        "                    hbr_wavelet = calculate_wavelet_coeffs(hbr_signal)\n",
        "                    hbr_wavelet_mean = np.mean(hbr_wavelet)\n",
        "\n",
        "                    node_features = [\n",
        "                        hbo_max, hbo_min, hbo_mean, hbo_std, hbo_slope, hbo_wavelet_mean,\n",
        "                        hbr_max, hbr_min, hbr_mean, hbr_std, hbr_slope, hbr_wavelet_mean\n",
        "                    ]\n",
        "\n",
        "                    x.append(node_features)\n",
        "\n",
        "                G = nx.Graph()\n",
        "\n",
        "                for m in range(len(channels_of_interest)):\n",
        "                    hbo_signal = hbo_window_data[:, m]\n",
        "                    hbr_signal = hbr_window_data[:, m]\n",
        "                    covariance = calculate_covariance(hbo_signal, hbr_signal)\n",
        "                    pli = calculate_phase_lag_index(hbo_signal, hbr_signal)\n",
        "                    pearson_corr = calculate_pearson_correlation(hbo_signal, hbr_signal)\n",
        "                    cross_corr = calculate_cross_correlation(hbo_signal, hbr_signal)\n",
        "\n",
        "                    edge_features = [covariance, pli, pearson_corr, cross_corr]\n",
        "\n",
        "                    edge_index.append([m, m])\n",
        "                    edge_attr.append(edge_features)\n",
        "                    G.add_edge(m, m, weight=pearson_corr)\n",
        "\n",
        "                    for n in range(m + 1, len(channels_of_interest)):\n",
        "                        if not np.all(hbo_window_data[:, m] == 0) and not np.all(hbo_window_data[:, n] == 0) and \\\n",
        "                           not np.all(hbr_window_data[:, m] == 0) and not np.all(hbr_window_data[:, n] == 0):\n",
        "                            pearson_corr = calculate_pearson_correlation(hbo_window_data[:, m], hbo_window_data[:, n])\n",
        "\n",
        "                            if abs(pearson_corr) > 0.60:  # Edge threshold check\n",
        "                                covariance = calculate_covariance(hbo_window_data[:, m], hbo_window_data[:, n])\n",
        "                                pli = calculate_phase_lag_index(hbo_window_data[:, m], hbo_window_data[:, n])\n",
        "                                cross_corr = calculate_cross_correlation(hbo_window_data[:, m], hbo_window_data[:, n])\n",
        "\n",
        "                                edge_features = [covariance, pli, pearson_corr, cross_corr]\n",
        "\n",
        "                                edge_index.append([m, n])\n",
        "                                edge_index.append([n, m])\n",
        "                                edge_attr.append(edge_features)\n",
        "                                G.add_edge(m, n, weight=pearson_corr)\n",
        "                                G.add_edge(n, m, weight=pearson_corr)\n",
        "                small_worldness = calculate_small_worldness(G)\n",
        "                modularity = calculate_modularity(G)\n",
        "                global_efficiency = calculate_global_efficiency(G)\n",
        "                avg_clustering_coeff = calculate_avg_clustering_coefficient(G)\n",
        "                eigenvector_centrality = calculate_eigenvector_centrality(G)\n",
        "\n",
        "                graph_features = [\n",
        "                    small_worldness if not np.isnan(small_worldness) else -1,\n",
        "                    modularity,\n",
        "                    global_efficiency,\n",
        "                    avg_clustering_coeff,\n",
        "                    eigenvector_centrality\n",
        "                ]\n",
        "\n",
        "                x = torch.tensor(x, dtype=torch.float)\n",
        "                edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "                edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
        "                graph_features = torch.tensor(graph_features, dtype=torch.float)\n",
        "\n",
        "                # Label each snapshot with 0\n",
        "                data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=torch.tensor([1]))\n",
        "                data.graph_features = graph_features\n",
        "\n",
        "                temporal_graphs.append(data)\n",
        "\n",
        "                pbar.update(1)\n",
        "                i += window_size - overlap\n",
        "\n",
        "            pbar.close()\n",
        "\n",
        "            # Save the sequence of temporal graphs for each patient\n",
        "            temporal_data = StaticGraphTemporalSignal(\n",
        "                edge_index=[g.edge_index for g in temporal_graphs],\n",
        "                edge_weight=[g.edge_attr for g in temporal_graphs],\n",
        "                features=[g.x for g in temporal_graphs],\n",
        "                targets=[g.y for g in temporal_graphs]  # Adjust the target as per your requirement\n",
        "            )\n",
        "\n",
        "            output_file_path = os.path.join(output_directory, f'{base_failename}_temporal_graphs.pt')\n",
        "            torch.save(temporal_data, output_file_path)\n",
        "\n",
        "print(\"Temporal graph data has been saved as .pt files for each patient.\")\n",
        "\n"
      ]
    }
  ]
}